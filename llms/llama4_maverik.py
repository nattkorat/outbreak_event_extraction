import os
from openai import OpenAI
from dotenv import load_dotenv

env_path = os.path.join(os.path.dirname(__file__), '.env')
# load the env file
load_dotenv(dotenv_path=env_path)

client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=os.getenv("OPEN_ROUTER_KEY"),
)

def chat_with_llama4_maverik(text: str) -> str:
    """
    Function to interact with Llama 4 Maverik model for text generation.
    
    Args:
        text (str): The input text to be processed by the model.
        
    Returns:
        str: The response generated by the model.
    """
    response = client.chat.completions.create(
        model="meta-llama/llama-4-maverick-17b-128e-instruct:free",
        temperature=0.0,
        messages=[
            {
              "role": "system",
              "content": "No explain, just return the final answer in JSON format."
            },
            {
                "role": "user",
                "content": text
            }
        ]
    )
    
    return response.choices[0].message.content

if __name__ == "__main__":
    # Example usage
    example_text = "What is the capital of France?"
    response = chat_with_llama4_maverik(example_text)
    print(f"Response from Llama 4 Maverik: {response}")